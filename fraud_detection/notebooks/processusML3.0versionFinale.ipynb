{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47230939",
   "metadata": {},
   "source": [
    "**DETECTION DE FRAUDES FINANCIERES** \\\n",
    "**ANDRIANTAOLO Valisoaniony Anouchka ** \\\n",
    "Version 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60430ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import random\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as mk \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed8a18",
   "metadata": {},
   "source": [
    "# 1. ANALYSE DE FORME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3903a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executez d'abord le notebook processus_ml1.0.ipynb\n",
    "df = pd.read_csv('../data/dataset_fraud_detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4319dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "for col in df.select_dtypes(include='int'):\n",
    "    df[col] = df[col].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a40388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6924041 entries, 0 to 6924040\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   Timestamp           datetime64[ns]\n",
      " 1   From Bank           object        \n",
      " 2   From Account        object        \n",
      " 3   To Bank             object        \n",
      " 4   To Account          object        \n",
      " 5   Amount Received     float64       \n",
      " 6   Receiving Currency  object        \n",
      " 7   Amount Paid         float64       \n",
      " 8   Payment Currency    object        \n",
      " 9   Payment Format      object        \n",
      "dtypes: datetime64[ns](1), float64(2), object(7)\n",
      "memory usage: 528.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc07dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacopy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d2b2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "From Bank",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "From Account",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "To Bank",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "To Account",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Amount Received",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Receiving Currency",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Amount Paid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Payment Currency",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Payment Format",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "5d5b31d1-6bc6-4e53-a3c4-32abe634c476",
       "rows": [
        [
         "count",
         "6924041",
         "6924041.0",
         "6924041",
         "6924041.0",
         "6924041",
         "6924041.0",
         "6924041",
         "6924041.0",
         "6924041",
         "6924041"
        ],
        [
         "unique",
         null,
         "41814.0",
         "681281",
         "21588.0",
         "576176",
         null,
         "15",
         null,
         "15",
         "7"
        ],
        [
         "top",
         null,
         "70.0",
         "10042B660",
         "11.0",
         "10042B660",
         null,
         "US Dollar",
         null,
         "US Dollar",
         "Cheque"
        ],
        [
         "freq",
         null,
         "609991.0",
         "222037",
         "66055.0",
         "1553",
         null,
         "2537242",
         null,
         "2553886",
         "2503158"
        ],
        [
         "mean",
         "2022-09-05 07:09:11.304066560",
         null,
         null,
         null,
         null,
         "6324073.911975413",
         null,
         "4676041.375337785",
         null,
         null
        ],
        [
         "min",
         "2022-09-01 00:00:00",
         null,
         null,
         null,
         null,
         "1e-06",
         null,
         "1e-06",
         null,
         null
        ],
        [
         "25%",
         "2022-09-02 04:26:00",
         null,
         null,
         null,
         null,
         "174.21",
         null,
         "175.38",
         null,
         null
        ],
        [
         "50%",
         "2022-09-05 12:12:00",
         null,
         null,
         null,
         null,
         "1397.63",
         null,
         "1399.45",
         null,
         null
        ],
        [
         "75%",
         "2022-09-08 03:04:00",
         null,
         null,
         null,
         null,
         "12296.53",
         null,
         "12226.87",
         null,
         null
        ],
        [
         "max",
         "2022-09-17 15:28:00",
         null,
         null,
         null,
         null,
         "3644853662746.95",
         null,
         "3644853662746.95",
         null,
         null
        ],
        [
         "std",
         null,
         null,
         null,
         null,
         null,
         "2105372355.8554296",
         null,
         "1544099398.697399",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 11
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>From Account</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>To Account</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6924041</td>\n",
       "      <td>6924041.0</td>\n",
       "      <td>6924041</td>\n",
       "      <td>6924041.0</td>\n",
       "      <td>6924041</td>\n",
       "      <td>6.924041e+06</td>\n",
       "      <td>6924041</td>\n",
       "      <td>6.924041e+06</td>\n",
       "      <td>6924041</td>\n",
       "      <td>6924041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>41814.0</td>\n",
       "      <td>681281</td>\n",
       "      <td>21588.0</td>\n",
       "      <td>576176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10042B660</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10042B660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>609991.0</td>\n",
       "      <td>222037</td>\n",
       "      <td>66055.0</td>\n",
       "      <td>1553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2537242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2553886</td>\n",
       "      <td>2503158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-09-05 07:09:11.304066560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.324074e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.676041e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2022-09-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2022-09-02 04:26:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.742100e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.753800e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-09-05 12:12:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.397630e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.399450e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022-09-08 03:04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.229653e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.222687e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2022-09-17 15:28:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.644854e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.644854e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.105372e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.544099e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Timestamp  From Bank From Account    To Bank  \\\n",
       "count                         6924041  6924041.0      6924041  6924041.0   \n",
       "unique                            NaN    41814.0       681281    21588.0   \n",
       "top                               NaN       70.0    10042B660       11.0   \n",
       "freq                              NaN   609991.0       222037    66055.0   \n",
       "mean    2022-09-05 07:09:11.304066560        NaN          NaN        NaN   \n",
       "min               2022-09-01 00:00:00        NaN          NaN        NaN   \n",
       "25%               2022-09-02 04:26:00        NaN          NaN        NaN   \n",
       "50%               2022-09-05 12:12:00        NaN          NaN        NaN   \n",
       "75%               2022-09-08 03:04:00        NaN          NaN        NaN   \n",
       "max               2022-09-17 15:28:00        NaN          NaN        NaN   \n",
       "std                               NaN        NaN          NaN        NaN   \n",
       "\n",
       "       To Account  Amount Received Receiving Currency   Amount Paid  \\\n",
       "count     6924041     6.924041e+06            6924041  6.924041e+06   \n",
       "unique     576176              NaN                 15           NaN   \n",
       "top     10042B660              NaN          US Dollar           NaN   \n",
       "freq         1553              NaN            2537242           NaN   \n",
       "mean          NaN     6.324074e+06                NaN  4.676041e+06   \n",
       "min           NaN     1.000000e-06                NaN  1.000000e-06   \n",
       "25%           NaN     1.742100e+02                NaN  1.753800e+02   \n",
       "50%           NaN     1.397630e+03                NaN  1.399450e+03   \n",
       "75%           NaN     1.229653e+04                NaN  1.222687e+04   \n",
       "max           NaN     3.644854e+12                NaN  3.644854e+12   \n",
       "std           NaN     2.105372e+09                NaN  1.544099e+09   \n",
       "\n",
       "       Payment Currency Payment Format  \n",
       "count           6924041        6924041  \n",
       "unique               15              7  \n",
       "top           US Dollar         Cheque  \n",
       "freq            2553886        2503158  \n",
       "mean                NaN            NaN  \n",
       "min                 NaN            NaN  \n",
       "25%                 NaN            NaN  \n",
       "50%                 NaN            NaN  \n",
       "75%                 NaN            NaN  \n",
       "max                 NaN            NaN  \n",
       "std                 NaN            NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1bc07",
   "metadata": {},
   "source": [
    "# 2. ANALYSE DU FOND (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES QUALITATIVES\n",
    "cat_cols = df.select_dtypes('object').columns\n",
    "vars_quals = df[cat_cols]\n",
    "vars_quals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7644da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES QUANTITATIVES\n",
    "cont_cols = df.select_dtypes('float').columns\n",
    "vars_quants = df[cont_cols]\n",
    "vars_quants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c7342",
   "metadata": {},
   "source": [
    "## 2.1. ANALYSE UNIVARIEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd50ee",
   "metadata": {},
   "source": [
    "### 2.1.1. Variables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALEURS UNIQUES\n",
    "for col in cat_cols :\n",
    "    print(f\"Colonne : {col :.<50} {df[col].nunique()} valeurs uniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c487f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODALITE ET FREQUENCE\n",
    "for col in cat_cols:\n",
    "    if df[col].nunique() > 15 :\n",
    "        fig, axes = plt.subplots(1,2, figsize=(12,8))\n",
    "        # identifier les banques/comptes les plus actifs et les moins actifs\n",
    "        top10 = df[col].value_counts(normalize=True).nlargest(10)\n",
    "        bottom10 = df[col].value_counts(normalize=True).nsmallest(10)\n",
    "        y_max = max(top10.max(),bottom10.max())*1.1\n",
    "        axtop = top10.plot(kind='bar', ax=axes[0],\n",
    "                           title=f'{col} (les 10 plus actifs)',\n",
    "                           ylim =(0,y_max),\n",
    "                           xlabel=\"Modalité\",\n",
    "                           ylabel=\"Fréquence\")\n",
    "        axbottom = bottom10.plot(kind='bar',ax=axes[1],\n",
    "                                 title=f'{col} (les 10 moins actifs)',\n",
    "                                 ylim =(0,y_max),\n",
    "                                 xlabel=\"Modalité\",\n",
    "                                 ylabel=\"Fréquence\")\n",
    "        for p1 in axtop.patches:\n",
    "            axtop.annotate(f'{p1.get_height():.2%}', (p1.get_x()+ p1.get_width()/2.,p1.get_height()), ha= 'center', va='center', xytext=(0,10), textcoords='offset points')\n",
    "        for p2 in axbottom.patches:\n",
    "            axbottom.annotate(f'{p2.get_height():.2%}', (p2.get_x()+ p2.get_width()/2.,p2.get_height()), ha= 'center', va='center', xytext=(0,10), textcoords='offset points')\n",
    "    else:\n",
    "        #  identifier les modalités dominantes et les valeurs rares potentiellement anormales\n",
    "        plt.figure(figsize=(10,8))\n",
    "        ax = df[col].value_counts(normalize=True).plot(kind='bar', title=f'{col}',\n",
    "                                                       xlabel=\"Modalité\",\n",
    "                                                       ylabel=\"Fréquence\",\n",
    "                                                       figsize=(10,8))\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{p.get_height():.2%}', (p.get_x()+ p.get_width()/2.,p.get_height()), ha= 'center', va='center', xytext=(0,10), textcoords='offset points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdcbc4",
   "metadata": {},
   "source": [
    "### 2.1.2. Variables quantitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAMME : pour regarder la forme --> la symétrie, l'applatissement\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,8))\n",
    "for i,col in enumerate(cont_cols) :\n",
    "    sns.histplot(df[col], bins=30, kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'{col}')\n",
    "    axes[i].set_xlabel('Montants')\n",
    "    axes[i].set_ylabel('Fréquence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXPLOT : pour les valeurs aberrantes --> valeurs extrêmes\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "sns.boxplot(data=vars_quants)\n",
    "plt.title('Boxplot des montants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc319de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQPLOT : pour savoir s'il s'agit d'une distribution normale --> comparer à une loi normale\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,8))\n",
    "for i, col in enumerate(cont_cols):\n",
    "    stats.probplot(df[col], dist='norm', plot=axes[i])\n",
    "    axes[i].set_title(f'{col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e404f4",
   "metadata": {},
   "source": [
    "## 2.2. ANALYSE BIVARIEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ccd08",
   "metadata": {},
   "source": [
    "### 2.2.1. Variables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e68fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCATTER PLOT : pour voir le type de relation --> linéaire ou non\n",
    "# Amount Paid vs Amount Received --> scatter plot + diagonale = montant inchangé\n",
    "max_val = max(df['Amount Paid'].max(), df['Amount Received'].max())\n",
    "\n",
    "plt.figure(figsize =(8,6))\n",
    "plt.scatter(df['Amount Paid'], df['Amount Received'], alpha=0.4)\n",
    "plt.plot([0, max_val],[0, max_val], color='red', ls='--', label='Amount Paid = Amount Received')\n",
    "plt.xlabel('Amount Paid')\n",
    "plt.ylabel('Amount Received')\n",
    "plt.title('Amount Paid vs Amount Received')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCATTER PLOT : pour voir si les écarts augmentent avec le montant\n",
    "# Delta vs Amount Paid/Received\n",
    "df['Delta'] = df['Amount Paid'] - df['Amount Received']\n",
    "\n",
    "for col in cont_cols :\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(df[col], df['Delta'], alpha=0.4)\n",
    "    plt.axhline(0, color='red', ls='--')\n",
    "    plt.xlabel(f'{col}')\n",
    "    plt.ylabel(f'Delta')\n",
    "    plt.title(f'Delta vs {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEATMAP : correlation --> relation linéaire mais la normalité n'est pas vérifiée\n",
    "sns.heatmap(vars_quants.corr(method='spearman'), annot=True, cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a097aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DE SPEARMAN\n",
    "\n",
    "# H0 : il n'y a pas de corrélation monotone entre les deux variables (pvalue >= 0.05)\n",
    "# H1 : il existe une corrélation monotone significative entre les deux variables (pvalue < 0.05)\n",
    "stats.spearmanr(vars_quants['Amount Paid'],vars_quants['Amount Received'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39645780",
   "metadata": {},
   "source": [
    "### 2.2.2. Variables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ce425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From Bank vs To Bank\"\"\"\n",
    "# pour la répartition croisée entre banque --> canaux bancaires les plus utilisés\n",
    "# association les plus fréquentes entre banques\n",
    "top10_from_bank = vars_quals[vars_quals['From Bank'].isin(vars_quals['From Bank'].value_counts().nlargest(10).index)]\n",
    "top10_to_bank = vars_quals[vars_quals['To Bank'].isin(vars_quals['To Bank'].value_counts().nlargest(10).index)]\n",
    "\n",
    "# HEATMAP\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(pd.crosstab(top10_from_bank['From Bank'],top10_to_bank['To Bank']), cmap='YlGnBu', annot=True, fmt='d')\n",
    "plt.title('Flux bancaires : From Bank vers To Bank (sur les top 10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From Account vs To Account\"\"\"\n",
    "# pour la répartition croisée entre compte--> comptes toujours connectés\n",
    "# association suspecte entre comptes fréquents\n",
    "top20_from_account = vars_quals[vars_quals['From Account'].isin(vars_quals['From Account'].value_counts().nlargest(20).index)]\n",
    "top20_to_account = vars_quals[vars_quals['To Account'].isin(vars_quals['To Account'].value_counts().nlargest(20).index)]\n",
    "\n",
    "# HEATMAP\n",
    "sns.heatmap(pd.crosstab(top20_from_account['From Account'],top20_to_account['To Account']), cmap='YlGnBu', annot=True, fmt='d')\n",
    "plt.title('Connexion des comptes : From Account vers To Account (sur les top 20)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de récepteurs de chaque compte --> envoie massive\n",
    "to_account_by_top20_from_account = vars_quals.groupby('From Account')['To Account'].nunique().nlargest(20)\n",
    "\n",
    "# BARPLOT\n",
    "plt.figure()\n",
    "sns.barplot(x=to_account_by_top20_from_account.index, y=to_account_by_top20_from_account.values)\n",
    "plt.title('Nombre de récepteurs par From Account (top 20)')\n",
    "plt.ylabel('Nombre (To Account)')\n",
    "plt.xticks(rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'émetteurs de chaque compte --> réception massive\n",
    "from_account_by_top20_to_account = vars_quals.groupby('To Account')['From Account'].nunique().nlargest(20)\n",
    "\n",
    "# BARPLOT\n",
    "plt.figure()\n",
    "sns.barplot(x=from_account_by_top20_to_account.index, y=from_account_by_top20_to_account.values)\n",
    "plt.title('Nombre d\\'émetteurs par To Account (top 20)')\n",
    "plt.ylabel('Nombre (From Account)')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c970e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From Bank ve From/To Account\"\"\"\n",
    "\n",
    "# compte lié par banque émettrice\n",
    "from_account_by_top20_from_bank = vars_quals.groupby('From Bank')['From Account'].nunique().nlargest(20)\n",
    "to_account_by_top20_from_bank = vars_quals.groupby('From Bank')['To Account'].nunique().nlargest(20)\n",
    "\n",
    "# BARPLOT\n",
    "fig, axes = plt.subplots(1,2, figsize=(8,6))\n",
    "\n",
    "# combien de comptes une banque utilise pour émettre\n",
    "sns.barplot(x=from_account_by_top20_from_bank.index, y=from_account_by_top20_from_bank.values, ax = axes[0],\n",
    "            order=from_account_by_top20_from_bank.index)\n",
    "\n",
    "# combien de comptes bénéficiaires ont reçu de l'argent\n",
    "sns.barplot(x=to_account_by_top20_from_bank.index, y=to_account_by_top20_from_bank.values, ax = axes[1],\n",
    "            order=to_account_by_top20_from_bank.index)\n",
    "\n",
    "y_max = max(from_account_by_top20_from_bank.max(), to_account_by_top20_from_bank.max())*1.1\n",
    "\n",
    "axes[0].set_title('From Bank <--> From Account')\n",
    "axes[0].set_ylabel('Nombre de comptes (From Account)')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "axes[0].set_ylim(0,y_max)\n",
    "\n",
    "axes[1].set_title('From Bank <--> To Account')\n",
    "axes[1].set_ylabel('Nombre de comptes (To Account)')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "axes[1].set_ylim(0,y_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a130f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To Bank vs From/To Account\"\"\"\n",
    "\n",
    "# compte lié par banque réceptrice\n",
    "from_account_by_top20_to_bank = vars_quals.groupby('To Bank')['From Account'].nunique().nlargest(20)\n",
    "to_account_by_top20_to_bank = vars_quals.groupby('To Bank')['To Account'].nunique().nlargest(20)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(8,6))\n",
    "\n",
    "# combien de comptes ont envoyé de l'argent\n",
    "sns.barplot(x=from_account_by_top20_to_bank.index, y=from_account_by_top20_to_bank.values, ax = axes[0],\n",
    "            order=from_account_by_top20_to_bank.index)\n",
    "\n",
    "# combien de comptes une banque héberge en réception\n",
    "sns.barplot(x=to_account_by_top20_to_bank.index, y=to_account_by_top20_to_bank.values, ax = axes[1],\n",
    "            order=to_account_by_top20_to_bank.index)\n",
    "\n",
    "y_max = max(from_account_by_top20_to_bank.max(), to_account_by_top20_to_bank.max())*1.1\n",
    "\n",
    "axes[0].set_title('To Bank <--> From Account')\n",
    "axes[0].set_ylabel('Nombre de comptes (From Account)')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "axes[0].set_ylim(0,y_max)\n",
    "\n",
    "axes[1].set_title('To Bank <--> To Account')\n",
    "axes[1].set_ylabel('Nombre de comptes (To Account)')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "axes[1].set_ylim(0,y_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39024a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Payment Currency vs Receiving Currency\"\"\"\n",
    "# pour la répartition croisée entre devise --> conversion\n",
    "# paire de devises inhabituelles\n",
    "\n",
    "# HEATMAP\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(pd.crosstab(vars_quals['Payment Currency'],vars_quals['Receiving Currency']), cmap='YlGnBu', annot=True, fmt='d')\n",
    "plt.title('Payment Currency vers Receiving Currency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74420a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Source : From Bank/Payment Format vs Payment Currency\"\"\"\n",
    "\n",
    "top10_from_bank = vars_quals[vars_quals['From Bank'].isin(vars_quals['From Bank'].value_counts().nlargest(10).index)]\n",
    "\n",
    "# COUNTPLOT\n",
    "for value in top10_from_bank['Payment Currency'].value_counts().index :\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.countplot(data=top10_from_bank[top10_from_bank['Payment Currency'] == value],\n",
    "                  x='From Bank', hue='Payment Format', order=vars_quals['From Bank'].value_counts().nlargest(10).index )\n",
    "    plt.title(f'banque/format : {value}')\n",
    "    plt.ylabel('Nombres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661afba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"destinataire : To Bank/Payment Format vs Receiving Currency\"\"\"\n",
    "\n",
    "top10_to_bank = vars_quals[vars_quals['To Bank'].isin(vars_quals['To Bank'].value_counts().nlargest(10).index)]\n",
    "\n",
    "# COUNTPLOT\n",
    "for value in top10_to_bank['Receiving Currency'].value_counts().index :\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.countplot(data=top10_to_bank[top10_to_bank['Receiving Currency'] == value],\n",
    "                  x='To Bank', hue='Payment Format', order=vars_quals['To Bank'].value_counts().nlargest(10).index )\n",
    "    plt.title(f'banque/format : {value}')\n",
    "    plt.ylabel('Nombres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Payment Format vs Payment/Received Currency\"\"\"\n",
    "# incohérence entre format et devise\n",
    "\n",
    "# BARPLOT\n",
    "pd.crosstab(vars_quals['Payment Currency'], vars_quals['Payment Format']).reindex(vars_quals['Payment Currency'].value_counts().index).plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "plt.title('Format de paiement par devise payée')\n",
    "plt.ylabel('Nombres')\n",
    "\n",
    "pd.crosstab(vars_quals['Receiving Currency'], vars_quals['Payment Format']).reindex(vars_quals['Receiving Currency'].value_counts().index).plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "plt.title('Format de paiement par devise reçue')\n",
    "plt.ylabel('Nombres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914df60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DE CHI2\n",
    "\n",
    "# H0 : il n'y a pas d'association entre les deux variables (pvalue >= 0.05)\n",
    "# H1 : il y a une association entre les deux variables (pvalue < 0.05)\n",
    "\n",
    "results =[]\n",
    "\n",
    "for col1, col2 in list(itertools.combinations(cat_cols,2)) :\n",
    "    if vars_quals[col1].nunique() > 15 :\n",
    "        top10_col1 = vars_quals[vars_quals[col1].isin(vars_quals[col1].value_counts().nlargest(30).index)]\n",
    "        # vars_quals[col1] = vars_quals[vars_quals[col1].isin(vars_quals[col1].value_counts().nlargest(20).index)][col1]\n",
    "        if vars_quals[col2].nunique() > 15 :\n",
    "            top10_col2 = vars_quals[vars_quals[col2].isin(vars_quals[col2].value_counts().nlargest(30).index)]\n",
    "            contingency = pd.crosstab(top10_col1[col1], top10_col2[col2])\n",
    "        else :\n",
    "            contingency = pd.crosstab(top10_col1[col1], vars_quals[col2])\n",
    "    else :\n",
    "        contingency = pd.crosstab(vars_quals[col1], vars_quals[col2])\n",
    "\n",
    "    # plt.figure()\n",
    "    # sns.heatmap(contingency)\n",
    "\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency)\n",
    "\n",
    "    results.append({\n",
    "        'Variable 1' : col1,\n",
    "        'Variable 2' : col2,\n",
    "        'Statistique du Chi2': chi2,\n",
    "        'p-valeur' : p,\n",
    "        'Degrés de liberté' : dof,\n",
    "        'Significative' : p < 0.05,\n",
    "        # 'Fréquences attendues' : expected\n",
    "    })\n",
    "\n",
    "    # si significative, voir la contribution et intensité de la relation (T de Tschuprow)\n",
    "    if p < 0.05 :\n",
    "        print(f'\\n --- Analyse : {col1} vs {col2} ---')\n",
    "\n",
    "        # T de Tschuprow\n",
    "        n = contingency.sum().sum()\n",
    "        min_dim = min(contingency.shape) - 1\n",
    "        tschuprow_t = np.sqrt(chi2 / (n * min_dim))\n",
    "        print(f'Coefficient T de Tschuprow  entre {col1} et {col2}: {tschuprow_t}')\n",
    "\n",
    "        # contribution\n",
    "        contrib = (contingency - expected)**2 / expected\n",
    "        contrib_percent = 100 * contrib / chi2\n",
    "\n",
    "        # contribution individuelle\n",
    "        contrib_flat = contrib_percent.stack().reset_index()\n",
    "        contrib_flat.columns = [col1, col2, 'Contribution (%)']\n",
    "\n",
    "        # extraire que les plus gros contributeurs > 5%\n",
    "        contrib_flat = contrib_flat[contrib_flat['Contribution (%)'] > 5]\n",
    "\n",
    "        contrib_flat = contrib_flat.sort_values(by='Contribution (%)', ascending=False)\n",
    "        display(contrib_flat)\n",
    "\n",
    "print('\\n')\n",
    "print(f'\\n ------ TEST DE CHI2 -----')\n",
    "chi2_results = pd.DataFrame(results).sort_values(by='p-valeur')\n",
    "display(chi2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bf758",
   "metadata": {},
   "source": [
    "### 2.2.3. Variables quatitatives et qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXPLOT\n",
    "for col1 in cat_cols:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,8))\n",
    "    if df[col1].nunique() > 15 :\n",
    "        top10 = df[df[col1].isin(df[col1].value_counts().nlargest(10).index)]\n",
    "        for i, col2 in enumerate(cont_cols):\n",
    "            sns.boxplot(data=top10, x=col1, y=col2, ax = axes[i])\n",
    "            axes[i].tick_params(axis='x', rotation=90)\n",
    "    else :\n",
    "        for i, col2 in enumerate(cont_cols):\n",
    "            sns.boxplot(data=df, x=col1, y=col2, ax=axes[i])\n",
    "            axes[i].tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DE KRUSKAL-WALLIS\n",
    "\n",
    "# H0 : les distributions de tous les groupes sont égales (pvalue >= 0.05)\n",
    "# H1 : au moins une des distributions des groupes est différentes des autres (pvalue < 0.05)\n",
    "\n",
    "results = []\n",
    "\n",
    "for cat in cat_cols :\n",
    "    for num in cont_cols :\n",
    "        # extraire les groupes selon la variable catégorielle\n",
    "        groups =[group[num].values for name, group in df.groupby(cat)]\n",
    "\n",
    "        # pour éviter d'avoir des groupes vides ou à un seul groupe\n",
    "        if len(groups) > 1 and all(len(g) > 0 for g in groups) :\n",
    "            k_stat, p = stats.kruskal(*groups)\n",
    "            results.append({\n",
    "                'Variable catégorielle' : cat,\n",
    "                'Variable numérique' : num,\n",
    "                'Statistique de Kruskal-Wallis': k_stat,\n",
    "                'p-valeur' : p,\n",
    "                'Significative' : p < 0.05,\n",
    "            })\n",
    "\n",
    "kruskal_results = pd.DataFrame(results).sort_values(by='p-valeur')\n",
    "display(kruskal_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35a4e9",
   "metadata": {},
   "source": [
    "### 2.2.4. Analyse temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b942a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulation temporelle\n",
    "df = df.sort_values('Timestamp')\n",
    "\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['Date'] = df['Timestamp'].dt.date\n",
    "df['Day'] = df['Timestamp'].dt.day_of_week\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809577f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOLUME DES TRANSACTIONS --> repérer des pics\n",
    "\n",
    "# LINEPLOT\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,8))\n",
    "for i,col in enumerate(cont_cols) :\n",
    "    df.groupby('Timestamp')[col].agg('count').resample('D').sum().plot(ax=axes[i], label='par date')\n",
    "    df.groupby('Timestamp')[col].agg('count').resample('h').sum().plot(ax=axes[i], label='par heure')\n",
    "    axes[i].set_title(f'{col}')\n",
    "    axes[i].set_xlabel('Période couverte (date et heure)')\n",
    "    axes[i].set_ylabel('Nombre des transactions')\n",
    "    axes[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7263f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONTANTS TOTALS --> repérer des pics\n",
    "\n",
    "# lineplot\n",
    "plt.figure(figsize=(10,8))\n",
    "df.groupby('Timestamp')['Amount Paid'].agg('sum').resample('D').sum().plot(label='Amount Paid par date')\n",
    "df.groupby('Timestamp')['Amount Received'].agg('sum').resample('D').sum().plot(label='Amount Received par date')\n",
    "df.groupby('Timestamp')['Amount Paid'].agg('sum').resample('h').sum().plot(label='Amount Paid par heure')\n",
    "df.groupby('Timestamp')['Amount Received'].agg('sum').resample('h').sum().plot(label='Amount Received par heure')\n",
    "plt.title(f'Montants totals des transactions')\n",
    "plt.xlabel('Période couverte (date et heure)')\n",
    "plt.ylabel('Montants totals')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de41980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAR HEURE --> repérer des activités\n",
    "\n",
    "# BARPLOT\n",
    "plt.figure()\n",
    "df.groupby('Hour')[cont_cols].agg('count').plot(kind='bar')\n",
    "plt.title('Volume des transactions par heure dans la journée')\n",
    "plt.xlabel('Période couverte (heure)')\n",
    "plt.ylabel('Nombre des transactions')\n",
    "\n",
    "plt.figure()\n",
    "df.groupby('Hour')[cont_cols].agg('sum').plot(kind='bar')\n",
    "plt.title('Montants totals par heure dans la journée')\n",
    "plt.xlabel('Période couverte (heure)')\n",
    "plt.ylabel('Montants totals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83367bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAR DATE --> repérer des activités\n",
    "\n",
    "# BARPLOT\n",
    "plt.figure()\n",
    "df.groupby('Date')[cont_cols].agg('count').plot(kind='bar')\n",
    "plt.title('Volume des transactions par date')\n",
    "plt.xlabel('Période couverte (date)')\n",
    "plt.ylabel('Nombre des transactions')\n",
    "\n",
    "plt.figure()\n",
    "df.groupby('Date')[cont_cols].agg('sum').plot(kind='bar')\n",
    "plt.title('Montants totals par date')\n",
    "plt.xlabel('Période couverte (date)')\n",
    "plt.ylabel('Montants totals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAR JOUR --> repérer des activités\n",
    "\n",
    "# BARPLOT\n",
    "plt.figure()\n",
    "df.groupby('Day')[cont_cols].agg('count').plot(kind='bar')\n",
    "plt.title('Volume des transactions par jour de la semaine')\n",
    "plt.xlabel('Période couverte (jour)')\n",
    "plt.ylabel('Nombre des transactions')\n",
    "\n",
    "plt.figure()\n",
    "df.groupby('Day')[cont_cols].agg('sum').plot(kind='bar')\n",
    "plt.title('Montants totals par jour de la semaine')\n",
    "plt.xlabel('Période couverte (jour)')\n",
    "plt.ylabel('Montants totals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e68b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERNS TEMPORELS (jour et heure) --> repérer des activités\n",
    "\n",
    "# HEATMAP\n",
    "sns.heatmap(df.groupby(['Day','Hour']).size().unstack(), cmap='YlGnBu')\n",
    "plt.title('Volume des transactions (jour x heure)')\n",
    "plt.xlabel('Heure')\n",
    "plt.ylabel('Jour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERNS TEMPORELS (date et heure) --> repérer des activités\n",
    "\n",
    "# HEATMAP\n",
    "sns.heatmap(df.groupby(['Date','Hour']).size().unstack(), cmap='YlGnBu')\n",
    "plt.title('Volume des transactions (date x heure)')\n",
    "plt.xlabel('Heure')\n",
    "plt.ylabel('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERNS TEMPORELS (date et jour) --> repérer des activités\n",
    "\n",
    "# HEATMAP\n",
    "sns.heatmap(df.groupby(['Date','Day']).size().unstack(), cmap='YlGnBu')\n",
    "plt.title('Volume des transactions (date x jour)')\n",
    "plt.xlabel('Jour')\n",
    "plt.ylabel('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTION EN FONCTION DU MOMENT --> anomalies temporelles périodiques (valeurs aaberrantes)\n",
    "\n",
    "# BOXPLOT\n",
    "sns.boxplot(data=vars_quants, x=df['Timestamp'], )\n",
    "plt.title('Distribution des transactions en fonction du temps')\n",
    "plt.xlabel('Période couverte (Date et heure)')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fac7d",
   "metadata": {},
   "source": [
    "# 3. PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc990af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test_streamlit = train_test_split(datacopy, test_size=0.3, random_state=0, shuffle=True)\n",
    "df_test, df_demo = train_test_split(df_test_streamlit, test_size=0.4, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0874b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : (4846828, 10)\n",
      "Test set : (1246327, 10)\n",
      "Demo set : (830886, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set : {df_train.shape}') # 70%\n",
    "print(f'Test set : {df_test.shape}') # 20%\n",
    "print(f'Demo set : {df_demo.shape}') # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f3a8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FONCTION POUR PREPARER LE DATASET à être transformé pour l'Autoencoder\n",
    "def preprocessing(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # feature extraction\n",
    "    df['Hour'] = df['Timestamp'].dt.hour\n",
    "    df['Day'] = df['Timestamp'].dt.day\n",
    "    df['DayOfWeek'] = df['Timestamp'].dt.dayofweek # 0 = lundi, 6 = dimanche\n",
    "    \n",
    "    # transformation log\n",
    "    df['Log_Amount_Paid'] = np.log1p(df['Amount Paid'])\n",
    "    df['Log_Amount_Diff'] = np.log1p(np.abs(df['Amount Paid'] - df['Amount Received']))\n",
    "    \n",
    "    # à supprimer\n",
    "    drop_cols = ['From Account', 'To Account','Amount Paid', 'Amount Received', 'Timestamp']\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    # From/To Bank : encodage par fréquence\n",
    "    for col in ['From Bank', 'To Bank'] :\n",
    "        freq = df[col].value_counts()\n",
    "        df[col] = df[col].map(freq).fillna(0)\n",
    "        df[col] = np.log1p(df[col])\n",
    "    \n",
    "    num_cols = df.select_dtypes(exclude='object').columns.tolist()\n",
    "    cat_cols = ['Receiving Currency', 'Payment Currency', 'Payment Format']\n",
    "    \n",
    "    return df, num_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 1 : TRANSFORMEUR\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline_lgbm = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc1bff",
   "metadata": {},
   "source": [
    "# 2. MODELISATION (Pipeline : transformeur + modèle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5e3f2",
   "metadata": {},
   "source": [
    "## Phase 1 : Détection d'anomalies (autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92042696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION DU MODELE AUTOENCODER (Pytorch)\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ce4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixer la reproductibilité pour l'autoencoder\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f94438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptation du modèle autoencoder pour être compatible avec le pipeline scikit-learn (fit, transform, predict)\n",
    "class AutoEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, epochs=20, batch_size=256, lr=1e-3, verbose=1, device=None, seed=0):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        set_seed(self.seed)\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        self.input_dim = X.shape[1]\n",
    "        self.model = Autoencoder(self.input_dim).to(self.device)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(torch.from_numpy(X))\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch_x, in loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = criterion(outputs, batch_x)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item() * batch_x.size(0)\n",
    "            epoch_loss /= len(loader.dataset)\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {epoch_loss:.6f}\")\n",
    "        return self\n",
    "\n",
    "    # retourne les erreurs de reconstruction (= score d’anomalie)\n",
    "    def score_samples(self, X):\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(X).to(self.device)\n",
    "            outputs = self.model(inputs).cpu().numpy()\n",
    "        errors = np.mean((X - outputs) ** 2, axis=1)\n",
    "        return errors\n",
    "\n",
    "    # pour compatibilité sklearn — retourne les scores\n",
    "    def transform(self, X):\n",
    "        return self.score_samples(X)\n",
    "\n",
    "    # Renvoie des labels binaires selon un seuil\n",
    "    def predict(self, X, threshold=None):\n",
    "        scores = self.score_samples(X)\n",
    "        if threshold is None:\n",
    "            threshold = np.percentile(scores, 99)\n",
    "        return (scores > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556d0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AE, num_cols, cat_cols = preprocessing(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f26c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE  2 : MODELE AUTOENCODER (transformeur + modèle)\n",
    "ae_pipeline = make_pipeline(\n",
    "    # encodage + scaling\n",
    "    ColumnTransformer(transformers=[\n",
    "        ('num', num_pipeline, num_cols),\n",
    "        ('cat', cat_pipeline, cat_cols)\n",
    "    ]),  \n",
    "    AutoEncoderWrapper(epochs=20, batch_size=256, verbose=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entraînement de l'autoencodeur\n",
    "ae_pipeline.fit(X_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du modèle autoencodeur entrâinée\n",
    "joblib.dump(ae_pipeline,'../model/ae_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b014de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si déjà entraîné et sauvegardé --> charger le modèle\n",
    "ae_pipeline = joblib.load('../model/ae_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AE['Anomaly_Score'] = ae_pipeline.score_samples(X_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = X_AE['Anomaly_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097529f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution des scores --> pour pouvoir  choisir le seuil à utiliser\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(scores, kde=True, bins=50)\n",
    "plt.title(\"Distribution des scores d'anomalie\")\n",
    "plt.xlabel(\"Score (erreur de reconstruction)\")\n",
    "plt.ylabel(\"Fréquence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marquer les plus anormaux (1% des scores les plus élévés)\n",
    "# --> seuil non paramétrique : quantile (position relative) car disttribution de loi spécifisue aux scores (empirique)\n",
    "threshold = np.percentile(scores, 99)\n",
    "X_AE['isAnomaly'] = ae_pipeline.predict(X_AE, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pourcentage des anomalies\n",
    "X_AE['isAnomaly'].value_counts(normalize=True).plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940033f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des scores avec seuil choisi\n",
    "plt.hist(scores, bins=100)\n",
    "plt.axvline(threshold, color='r', linestyle='--')\n",
    "plt.title(\"Distribution des scores d'anomalie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80473f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset des anomalies (sans transformation : revenir au dataset brute + anomalies) \n",
    "df_anomalies = df_train.copy()\n",
    "df_anomalies.loc[X_AE.index, 'isAnomaly'] = X_AE['isAnomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = df_anomalies[df_anomalies['isAnomaly'] == 1].drop(columns=['isAnomaly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc57537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment Format x anomalies\n",
    "anomalies['Payment Format'].value_counts().plot(kind='bar', ylabel='Fréquence', title='Payement Format (anomalies)')\n",
    "# anomalies['Payment Format'].value_counts().plot().pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c97cf1",
   "metadata": {},
   "source": [
    "## Phase 2 : Pseudo-labellisation (Clustering des anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_KMEANS, num_cols, cat_cols = preprocessing(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline global de transformation\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146596a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster = preprocessor.fit_transform(X_KMEANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed020244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHODE DU COUDE : détermination de k\n",
    "inertias = []\n",
    "K_range = range(1, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K_range, inertias, marker='o')\n",
    "plt.xlabel('Nombre de clusters K')\n",
    "plt.ylabel('Inertie (somme des distances intra-cluster)')\n",
    "plt.title(\"Méthode du coude (Elbow Method)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 3 : KMEANS (transformeur + modèle)\n",
    "\n",
    "kmeans_pipeline = make_pipeline(\n",
    "    ColumnTransformer(transformers=[\n",
    "         ('num', num_pipeline, num_cols),\n",
    "         ('cat', cat_pipeline, cat_cols)\n",
    "    ]), # encodage + scaling\n",
    "    KMeans(n_clusters=3, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd87466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entaîenement du kmeans\n",
    "kmeans_pipeline.fit(X_KMEANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f760e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(kmeans_pipeline, '../model/kmeans_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pipeline  = joblib.load('../model/kmeans_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d47bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans_pipeline.predict(X_KMEANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_KMEANS['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_KMEANS['Cluster'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = anomalies.copy()\n",
    "df_cluster.loc[X_KMEANS.index, 'Cluster'] = X_KMEANS['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9eea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41433ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FONCTION DE CREATION DES VARIABLES RFM (utile pour les signaux d'alarmes)\n",
    "def rfm_features(df):\n",
    "    df=df.copy()\n",
    "\n",
    "    #------ RECENCE (R) -------\n",
    "\n",
    "    df['Hour'] = df['Timestamp'].dt.hour\n",
    "    df['Day'] = df['Timestamp'].dt.day\n",
    "    df['DayOfWeek'] = df['Timestamp'].dt.dayofweek # 0 = lundi, 6 = dimanche\n",
    "\n",
    "        # récence compte émetteur : nombre de jours écoulés depuis la dernière transaction (From Account)\n",
    "    last_tx = df.groupby('From Account')['Timestamp'].transform('max')\n",
    "    df['Recency_Days'] = (df['Timestamp'].max() - last_tx).dt.days # récence de réference (From Account)\n",
    "    \n",
    "    df['isNight'] = df['Hour'].apply(lambda x: 1 if (x < 6 or x > 22) else 0)\n",
    "    df['isWeekend'] = df['DayOfWeek'].isin([5,6]).astype(int)\n",
    "\n",
    "\n",
    "    #------- FREQUENCE (F) ---------\n",
    "\n",
    "    # nombre de transactions par compte\n",
    "    df['Freq_Tx'] = df.groupby('From Account')['Timestamp'].transform('count') # transactions effectuées (réference : From Account)\n",
    "\n",
    "    # nombre de destinataires (To Account) uniques par compte (From Account)\n",
    "    df['Unique_To_per_From'] = df.groupby('From Account')['To Account'].transform('nunique')\n",
    "\n",
    "    # Brust : nombre de transactions dans une petite intervalle de temps (ex: < 5min)\n",
    "    Time_diff_Min = df.sort_values(['From Account', 'Timestamp']).groupby('From Account')['Timestamp'].diff().dt.total_seconds() / 60\n",
    "    df['isBrust'] = Time_diff_Min.apply(lambda x: int(x <=3 if pd.notnull(x) else 0))\n",
    "\n",
    "\n",
    "    #------ MONETAIRE (M) ------\n",
    "\n",
    "    # différence des montants payés et reçus (alternative stable pour ne garder qu'une seule des variables de base)\n",
    "    df['Amount_Diff'] = df['Amount Paid'] - df['Amount Received']\n",
    "\n",
    "    # moyenne et max des montants envoyées (From account)\n",
    "    df['Amount_Mean'] = df.groupby('From Account')['Amount Paid'].transform('mean')\n",
    "    df['Amount_Max'] = df.groupby('From Account')['Amount Paid'].transform('max')\n",
    "\n",
    "    # montants petits fréquents (smurfing)\n",
    "    df['Small_Amount'] = (df['Amount Paid'] < 200).astype(int)\n",
    "    df['Nb_Small_Tx'] = (df.groupby('From Account')['Small_Amount'].transform('sum'))\n",
    "\n",
    "    # transformation logarithmique pour stabiliser la distribution\n",
    "    df['Log_Amount_Paid'] = np.log1p(df['Amount Paid'])\n",
    "    df['Log_Amount_Diff'] = np.log1p(np.abs(df['Amount_Diff']))\n",
    "    df['Log_Amount_Mean'] = np.log1p(df['Amount_Mean'])\n",
    "    df['Log_Amount_Max'] = np.log1p(df['Amount_Max'])\n",
    "\n",
    "\n",
    "    #------- AUTRES -------\n",
    "    # pour garder le minimum d'information de From bank et To Bank\n",
    "    df['Same_Bank_Transfer'] = (df['From Bank'] == df['To Bank']).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signals_frauds(df, cluster_col='Cluster', confidence_threshold=0.05):\n",
    "    df = df.copy()\n",
    "\n",
    "    # SIGNAUX\n",
    "    df['High_Amount'] = ((df['Log_Amount_Mean'] > df['Log_Amount_Mean'].quantile(0.95)) |\n",
    "                         (df['Log_Amount_Max'] > df['Log_Amount_Max'].quantile(0.95))).astype(int)\n",
    "    df['isInternational'] = (df['Receiving Currency'] != df['Payment Currency']).astype(int)\n",
    "    df['Freq_Small_Tx'] = (df['Nb_Small_Tx'] > df['Nb_Small_Tx'].quantile(0.9)).astype(int)\n",
    "    df['Many_Dests'] = (df['Unique_To_per_From'] > df['Unique_To_per_From'].quantile(0.95)).astype(int)\n",
    "    df['High_Freq_From'] = (df['Freq_Tx'] > df['Freq_Tx'].quantile(0.95)).astype(int)\n",
    "    df['Similary'] = ((df['Receiving Currency'] == df['Payment Currency']) |\n",
    "                      (df.get('Same_Bank_Transfer', 0) == 1)).astype(int)\n",
    "    df['Wire_ACH_Bitcoin_Format'] = df['Payment Format'].isin(['Wire', 'Bitcoin', 'ACH']).astype(int)\n",
    "\n",
    "    df['Very_Recent'] = (df['Recency_Days'] < 1).astype(int)\n",
    "    df['Reactivation_Suspect'] = ((df['Recency_Days'] > 10) & (df['Freq_Tx'] > 2)).astype(int)\n",
    "    df['Cash_Bitcoin_Format'] = df['Payment Format'].isin(['Cash', 'Bitcoin']).astype(int)\n",
    "\n",
    "    df['Credit_Format'] = (df['Payment Format'] == 'Credit Card').astype(int)\n",
    "\n",
    "    frauds = {\n",
    "        'blanchiment': ['High_Amount','isInternational','Freq_Small_Tx','Many_Dests','High_Freq_From','Similary','Wire_ACH_Bitcoin_Format'],\n",
    "        'fraude par carte': ['Very_Recent','isNight','isWeekend', 'isBrust', 'Credit_Format'],\n",
    "        'fraude par compte mule': ['Very_Recent','Reactivation_Suspect','Many_Dests','High_Freq_From','Cash_Bitcoin_Format']\n",
    "    }\n",
    "\n",
    "    all_signals = list(set(sig for sigs in frauds.values() for sig in sigs))\n",
    "    existing_signals = [s for s in all_signals if s in df.columns]\n",
    "    cluster_profiles = df.groupby(cluster_col)[existing_signals].mean()\n",
    "\n",
    "    cluster_scores = []\n",
    "\n",
    "    # Calcul des scores possibles selon formats de paiement par cluster\n",
    "    for cluster_id, row in cluster_profiles.iterrows():\n",
    "        formats_present = set(df[df[cluster_col] == cluster_id]['Payment Format'].unique())\n",
    "\n",
    "        possible_types = set()\n",
    "        if any(f in formats_present for f in ['Wire', 'ACH', 'Bitcoin']):\n",
    "            possible_types.add('blanchiment')\n",
    "        if 'Credit Card' in formats_present:\n",
    "            possible_types.add('fraude par carte')\n",
    "        if any(f in formats_present for f in ['Cash', 'Bitcoin']):\n",
    "            possible_types.add('fraude par compte mule')\n",
    "\n",
    "        for fraud_type in possible_types:\n",
    "            signals = frauds[fraud_type]\n",
    "            present_signals = [s for s in signals if s in row.index]\n",
    "            if present_signals:\n",
    "                score = row[present_signals].mean()\n",
    "                cluster_scores.append((cluster_id, fraud_type, score))\n",
    "\n",
    "    # Trier par score décroissant\n",
    "    cluster_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    cluster_to_fraud = {}\n",
    "    assigned_types = set()\n",
    "    assigned_clusters = set()\n",
    "\n",
    "    # Attribution unique pour chaque type, au cluster avec meilleur score\n",
    "    for cluster_id, fraud_type, score in cluster_scores:\n",
    "        if fraud_type not in assigned_types and cluster_id not in assigned_clusters and score >= confidence_threshold:\n",
    "            cluster_to_fraud[cluster_id] = fraud_type\n",
    "            assigned_types.add(fraud_type)\n",
    "            assigned_clusters.add(cluster_id)\n",
    "\n",
    "    # Les clusters non attribués sont assignés au dernier type de fraude non attribué,\n",
    "    # sauf si leur score max est trop bas, alors \"normale\"\n",
    "    remaining_types = set(frauds.keys()) - assigned_types\n",
    "\n",
    "    for cluster_id in cluster_profiles.index:\n",
    "        if cluster_id not in assigned_clusters:\n",
    "            # Score max possible pour ce cluster\n",
    "            scores_for_cluster = [score for cid, _, score in cluster_scores if cid == cluster_id]\n",
    "            max_score = max(scores_for_cluster) if scores_for_cluster else 0\n",
    "\n",
    "            if max_score < confidence_threshold or not remaining_types:\n",
    "                cluster_to_fraud[cluster_id] = \"légitime\"\n",
    "            else:\n",
    "                # Attribuer un type restant (au hasard, ou mieux à celui avec meilleur score)\n",
    "                # Ici on prend le premier de remaining_types\n",
    "                fraud_type = remaining_types.pop()\n",
    "                cluster_to_fraud[cluster_id] = fraud_type\n",
    "                assigned_types.add(fraud_type)\n",
    "                assigned_clusters.add(cluster_id)\n",
    "\n",
    "    df['Pseudo_Labels'] = df[cluster_col].map(cluster_to_fraud)\n",
    "\n",
    "    # Affichage résumé\n",
    "    for cluster_id, fraud_type in cluster_to_fraud.items():\n",
    "        print(f\"Cluster {cluster_id} attribué à : {fraud_type}\")\n",
    "\n",
    "    return df, cluster_to_fraud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM = rfm_features(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58461c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frauds, mapping = signals_frauds(RFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_frauds, x='Pseudo_Labels', hue='Payment Format')\n",
    "plt.title('Répartition des formats de paiement par type de fraude')\n",
    "plt.xlabel('Type de fraude')\n",
    "plt.ylabel('Nombre de transactions')\n",
    "plt.xticks(rotation=15)\n",
    "plt.legend(title='Format de paiement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b059bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset avec les labels (sans transformation)\n",
    "df_labels = df_cluster.copy()\n",
    "df_labels.loc[df_frauds.index, 'Pseudo_Labels'] = df_frauds['Pseudo_Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c761a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels['Pseudo_Labels'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9473bd",
   "metadata": {},
   "source": [
    "## Phase 3 : Classification multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lgbm(df):\n",
    "    df = df.copy()\n",
    "    df, num_cols, cat_cols = preprocessing(df)\n",
    "    X = df.drop(columns='Pseudo_Labels')\n",
    "    y = df['Pseudo_Labels']\n",
    "    return X, y, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, cat_cols = preprocessing_lgbm(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be867939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 4 : LIGHTGBM\n",
    "lgbm_pipeline = make_pipeline(\n",
    "    ColumnTransformer(transformers=[\n",
    "    ('cat', cat_pipeline_lgbm, cat_cols)\n",
    "]),\n",
    "    lgbm.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=4,  # légitime + 3 fraudes\n",
    "        random_state=0,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'  # utile en cas de déséquilibre\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entraîenement de lightgbm\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "lgbm_pipeline.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du modèle lgbm entrâiné\n",
    "joblib.dump(lgbm_pipeline, '../model//lgbm_pipeline.pkl')\n",
    "joblib.dump(label_encoder, '../model/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf05e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_test(df):\n",
    "    df= df.copy()\n",
    "    X_AE, num_cols, cat_cols = preprocessing(df)\n",
    "    X_AE['isAnomaly'] = ae_pipeline.predict(X_AE, threshold = threshold)\n",
    "    df_anomalies = df.copy()\n",
    "    df_anomalies.loc[X_AE.index, 'isAnomaly'] = X_AE['isAnomaly']\n",
    "    anomalies = df_anomalies[df_anomalies['isAnomaly'] == 1].drop(columns='isAnomaly')\n",
    "    X_KMEANS, num_cols, cat_cols = preprocessing(anomalies)\n",
    "    X_KMEANS['Cluster'] = kmeans_pipeline.predict(X_KMEANS)\n",
    "    df_cluster = anomalies.copy()\n",
    "    df_cluster.loc[X_KMEANS.index, 'Cluster'] = X_KMEANS['Cluster']\n",
    "    RFM = rfm_features(df_cluster)\n",
    "    df_frauds, mapping = signals_frauds(RFM)\n",
    "    df_labels = df_cluster.copy()\n",
    "    df_labels.loc[df_frauds.index, 'Pseudo_Labels'] = df_frauds['Pseudo_Labels']\n",
    "    X_test, y_test, cat_cols = preprocessing_lgbm(df_labels)\n",
    "    return X_test, y_test, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b45ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, cat_cols = preprocessing_test(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_pred = lgbm_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(classification_report(y_test_encoded, y_pred))\n",
    "print(confusion_matrix(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtention des probabilités pour chaque classe pour l'ensemble d'entraînement et de test\n",
    "y_train_prob = lgbm_pipeline.predict_proba(X_train)\n",
    "y_test_prob = lgbm_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Classes uniques et encodées\n",
    "classes_encoded = label_encoder.classes_\n",
    "n_classes = len(classes_encoded)\n",
    "\n",
    "# Définir les couleurs pour chaque classe\n",
    "colors = plt.cm.get_cmap('viridis', n_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Calcul de la courbe ROC et de l'AUC pour chaque classe (One vs Rest)\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train_encoded, y_train_prob[:, i], pos_label=i)\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_encoded, y_test_prob[:, i], pos_label=i)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    color = colors(i)\n",
    "    class_name = classes_encoded[i]\n",
    "\n",
    "    # Plot ROC curve : training set (solid line)\n",
    "    plt.plot(fpr_train, tpr_train, color=color, lw=2,\n",
    "             label=f'ROC (Train) {class_name} (AUC = {roc_auc_train:.2f})')\n",
    "\n",
    "    # Plot ROC curve : test set (dashed line)\n",
    "    plt.plot(fpr_test, tpr_test, color=color, lw=2, linestyle='--',\n",
    "             label=f'ROC (Test) {class_name} (AUC = {roc_auc_test:.2f})')\n",
    "\n",
    "# Plot random guess line\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "# labels et title\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Faux positifs')\n",
    "plt.ylabel('Vrais positifs')\n",
    "plt.title('Receiver Operating Characteristic (ROC) par classe')\n",
    "plt.legend(loc=\"lower right\", bbox_to_anchor=(1.05, 0), borderaxespad=0.)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
